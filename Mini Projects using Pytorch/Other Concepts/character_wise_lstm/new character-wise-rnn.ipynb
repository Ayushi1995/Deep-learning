{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#from sklearn import datasets\n",
    "#import pixiedust\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/anna.txt','r') as f:\n",
    "#     text=f.read()\n",
    "with open('data/The_Alchemist.txt','r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Alchemist\\n Paulo Coelho\\nTranslated by Alan R. Clarke. Published 1992. ISBN 0-7225-3293-8.\\n\\n\\nCONT'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'y'), (1, '9'), (2, '`'), (3, 'c'), (4, '\\n'), (5, 'U'), (6, '2'), (7, 'f'), (8, '6'), (9, '3')]\n",
      "[('y', 0), ('9', 1), ('`', 2), ('c', 3), ('\\n', 4), ('U', 5), ('2', 6), ('f', 7), ('6', 8), ('3', 9)]\n"
     ]
    }
   ],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "\n",
    "chars=tuple(set(text))\n",
    "int2char= dict(enumerate(chars)) # key-> integers, value-> charac\n",
    "print(take(10, int2char.items()))\n",
    "char2int = {value:key for key, value in int2char.items()} # key-> characters, value-> integers\n",
    "print(take(10, char2int.items()))\n",
    "#encode the text\n",
    "encoded= np.array([char2int[ch] for ch in text])\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 71, 15, 47, 55, 60, 30, 79, 41,  4,  4,  4, 17, 15, 47, 47,  0,\n",
       "       79,  7, 15, 54, 76, 58, 76, 60, 50, 79, 15, 30, 60, 79, 15, 58, 58,\n",
       "       79, 15, 58, 76, 42, 60, 65, 79, 60, 59, 60, 30,  0, 79, 80, 29, 71,\n",
       "       15, 47, 47,  0, 79,  7, 15, 54, 76, 58,  0, 79, 76, 50, 79, 80, 29,\n",
       "       71, 15, 47, 47,  0, 79, 76, 29, 79, 76, 55, 50, 79, 61, 74, 29,  4,\n",
       "       74, 15,  0, 73,  4,  4, 27, 59, 60, 30,  0, 55, 71, 76, 29])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_classes):\n",
    "    #initialize\n",
    "    one_hot= np.zeros((arr.size,n_classes), dtype=np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]),arr.flatten()]=1.\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_classes))\n",
    "\n",
    "    return one_hot\n",
    "    \n",
    "    #fill appropriate with ones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = np.array([[0,3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAking Training mini-bataches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "#from IPython.core.debugger import set_trace\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    \n",
    "    batch_size_total= batch_size * seq_length\n",
    "    \n",
    "    n_batches= len(arr)//batch_size_total #floor division\n",
    "    \n",
    "    #keep only enough characters to make full batches\n",
    "    arr=arr[:n_batches * batch_size_total]\n",
    "    \n",
    "    #reshape into batch_size rows\n",
    "    arr=arr.reshape((batch_size,-1))\n",
    "    \n",
    "    #iterate throught the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        #the features\n",
    "        x=arr[:,n:n+seq_length]\n",
    "        #the targts shifted by 1\n",
    "        y=np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1],y[:,-1]=x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1], y[:,-1]=x[:, 1:], arr[:, 0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[1 2 3]\n",
      " [7 8 9]]\n",
      "y [[ 2  3  4]\n",
      " [ 8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "batch_size=2\n",
    "seq_length=3\n",
    "\n",
    "batches= get_batches(arr, batch_size, seq_length)\n",
    "\n",
    "\n",
    "x, y = next(batches)\n",
    "print('x',x)\n",
    "print('y',y)\n",
    "\n",
    "# batches = get_batches(encoded, 8, 50)\n",
    "# x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[35 71 15 47 55 60 30 79 41  4]\n",
      " [50 61 29 79 55 71 15 55 79 15]\n",
      " [60 29 38 79 61 30 79 15 79  7]\n",
      " [50 79 55 71 60 79  3 71 76 60]\n",
      " [79 50 15 74 79 71 60 30 79 55]\n",
      " [ 3 80 50 50 76 61 29 79 15 29]\n",
      " [79 44 29 29 15 79 71 15 38 79]\n",
      " [66 48 58 61 29 50 42  0 73 79]]\n",
      "\n",
      "y\n",
      " [[71 15 47 55 60 30 79 41  4  4]\n",
      " [61 29 79 55 71 15 55 79 15 55]\n",
      " [29 38 79 61 30 79 15 79  7 61]\n",
      " [79 55 71 60 79  3 71 76 60  7]\n",
      " [50 15 74 79 71 60 30 79 55 60]\n",
      " [80 50 50 76 61 29 79 15 29 38]\n",
      " [44 29 29 15 79 71 15 38 79 50]\n",
      " [48 58 61 29 50 42  0 73 79 63]]\n"
     ]
    }
   ],
   "source": [
    "#printing first 10 items of a sequence\n",
    "print(\"x\\n\",x[:10, :10])\n",
    "print(\"\\ny\\n\",y[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on gpu\n"
     ]
    }
   ],
   "source": [
    "# check if gpu available\n",
    "train_on_gpu= torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print(\"Training on gpu\")\n",
    "else:\n",
    "    print(\"no gpu available, trainig on CPU, consider making epochs very small,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2668... Val Loss: 3.2154\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1573... Val Loss: 3.1403\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1472... Val Loss: 3.1260\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1159... Val Loss: 3.1194\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1466... Val Loss: 3.1175\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1191... Val Loss: 3.1147\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1079... Val Loss: 3.1113\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1185... Val Loss: 3.1028\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1006... Val Loss: 3.0814\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0499... Val Loss: 3.0342\n",
      "Epoch: 1/20... Step: 110... Loss: 2.9701... Val Loss: 2.9425\n",
      "Epoch: 1/20... Step: 120... Loss: 2.8585... Val Loss: 2.8554\n",
      "Epoch: 1/20... Step: 130... Loss: 2.7692... Val Loss: 2.7337\n",
      "Epoch: 2/20... Step: 140... Loss: 2.6697... Val Loss: 2.6285\n",
      "Epoch: 2/20... Step: 150... Loss: 2.5947... Val Loss: 2.5301\n",
      "Epoch: 2/20... Step: 160... Loss: 2.5260... Val Loss: 2.4825\n",
      "Epoch: 2/20... Step: 170... Loss: 2.4553... Val Loss: 2.4338\n",
      "Epoch: 2/20... Step: 180... Loss: 2.4285... Val Loss: 2.4008\n",
      "Epoch: 2/20... Step: 190... Loss: 2.3839... Val Loss: 2.3868\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3686... Val Loss: 2.3417\n",
      "Epoch: 2/20... Step: 210... Loss: 2.3378... Val Loss: 2.3096\n",
      "Epoch: 2/20... Step: 220... Loss: 2.2962... Val Loss: 2.2830\n",
      "Epoch: 2/20... Step: 230... Loss: 2.2819... Val Loss: 2.2512\n",
      "Epoch: 2/20... Step: 240... Loss: 2.2663... Val Loss: 2.2307\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2016... Val Loss: 2.2056\n",
      "Epoch: 2/20... Step: 260... Loss: 2.1692... Val Loss: 2.1808\n",
      "Epoch: 2/20... Step: 270... Loss: 2.1871... Val Loss: 2.1595\n",
      "Epoch: 3/20... Step: 280... Loss: 2.1728... Val Loss: 2.1321\n",
      "Epoch: 3/20... Step: 290... Loss: 2.1453... Val Loss: 2.1088\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1247... Val Loss: 2.0876\n",
      "Epoch: 3/20... Step: 310... Loss: 2.0888... Val Loss: 2.0678\n",
      "Epoch: 3/20... Step: 320... Loss: 2.0591... Val Loss: 2.0492\n",
      "Epoch: 3/20... Step: 330... Loss: 2.0326... Val Loss: 2.0314\n",
      "Epoch: 3/20... Step: 340... Loss: 2.0605... Val Loss: 2.0133\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0377... Val Loss: 1.9985\n",
      "Epoch: 3/20... Step: 360... Loss: 1.9686... Val Loss: 1.9811\n",
      "Epoch: 3/20... Step: 370... Loss: 1.9938... Val Loss: 1.9627\n",
      "Epoch: 3/20... Step: 380... Loss: 1.9716... Val Loss: 1.9467\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9415... Val Loss: 1.9304\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9158... Val Loss: 1.9134\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9284... Val Loss: 1.9090\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9207... Val Loss: 1.8877\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9023... Val Loss: 1.8733\n",
      "Epoch: 4/20... Step: 440... Loss: 1.8932... Val Loss: 1.8635\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8342... Val Loss: 1.8477\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8161... Val Loss: 1.8376\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8569... Val Loss: 1.8239\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8375... Val Loss: 1.8121\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8348... Val Loss: 1.8021\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8284... Val Loss: 1.7953\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8148... Val Loss: 1.7815\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8184... Val Loss: 1.7704\n",
      "Epoch: 4/20... Step: 530... Loss: 1.7770... Val Loss: 1.7661\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7479... Val Loss: 1.7519\n",
      "Epoch: 4/20... Step: 550... Loss: 1.7892... Val Loss: 1.7434\n",
      "Epoch: 5/20... Step: 560... Loss: 1.7580... Val Loss: 1.7372\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7480... Val Loss: 1.7312\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7227... Val Loss: 1.7210\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7249... Val Loss: 1.7107\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7110... Val Loss: 1.7046\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7060... Val Loss: 1.6999\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7019... Val Loss: 1.6899\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7164... Val Loss: 1.6778\n",
      "Epoch: 5/20... Step: 640... Loss: 1.6927... Val Loss: 1.6750\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6778... Val Loss: 1.6684\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6541... Val Loss: 1.6593\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6838... Val Loss: 1.6567\n",
      "Epoch: 5/20... Step: 680... Loss: 1.6749... Val Loss: 1.6449\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6465... Val Loss: 1.6408\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6560... Val Loss: 1.6323\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6335... Val Loss: 1.6265\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6256... Val Loss: 1.6208\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6473... Val Loss: 1.6170\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6022... Val Loss: 1.6158\n",
      "Epoch: 6/20... Step: 750... Loss: 1.5910... Val Loss: 1.6082\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6364... Val Loss: 1.6014\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6142... Val Loss: 1.6017\n",
      "Epoch: 6/20... Step: 780... Loss: 1.5917... Val Loss: 1.5896\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5875... Val Loss: 1.5858\n",
      "Epoch: 6/20... Step: 800... Loss: 1.5950... Val Loss: 1.5810\n",
      "Epoch: 6/20... Step: 810... Loss: 1.5854... Val Loss: 1.5759\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5452... Val Loss: 1.5712\n",
      "Epoch: 6/20... Step: 830... Loss: 1.5958... Val Loss: 1.5664\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5499... Val Loss: 1.5672\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5695... Val Loss: 1.5591\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5458... Val Loss: 1.5519\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5505... Val Loss: 1.5512\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5696... Val Loss: 1.5470\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5548... Val Loss: 1.5394\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5335... Val Loss: 1.5386\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5047... Val Loss: 1.5323\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5404... Val Loss: 1.5276\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5244... Val Loss: 1.5251\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5289... Val Loss: 1.5239\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5349... Val Loss: 1.5217\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5293... Val Loss: 1.5134\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5398... Val Loss: 1.5119\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5152... Val Loss: 1.5111\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5164... Val Loss: 1.5055\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5040... Val Loss: 1.5010\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5320... Val Loss: 1.4984\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5022... Val Loss: 1.4974\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.4863... Val Loss: 1.4928\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.4908... Val Loss: 1.4958\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4762... Val Loss: 1.4886\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.4854... Val Loss: 1.4849\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.4878... Val Loss: 1.4785\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.4795... Val Loss: 1.4754\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4685... Val Loss: 1.4723\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4538... Val Loss: 1.4682\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4702... Val Loss: 1.4689\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.4741... Val Loss: 1.4675\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4706... Val Loss: 1.4641\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4693... Val Loss: 1.4610\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.4892... Val Loss: 1.4593\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4427... Val Loss: 1.4552\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4506... Val Loss: 1.4545\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4381... Val Loss: 1.4574\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.4819... Val Loss: 1.4542\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4276... Val Loss: 1.4484\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4315... Val Loss: 1.4445\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4351... Val Loss: 1.4460\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4082... Val Loss: 1.4401\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4224... Val Loss: 1.4335\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4372... Val Loss: 1.4366\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4314... Val Loss: 1.4354\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4353... Val Loss: 1.4318\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4348... Val Loss: 1.4315\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4288... Val Loss: 1.4331\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4236... Val Loss: 1.4293\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4339... Val Loss: 1.4260\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4007... Val Loss: 1.4267\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4058... Val Loss: 1.4224\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.3909... Val Loss: 1.4173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.3848... Val Loss: 1.4147\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.3938... Val Loss: 1.4163\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3811... Val Loss: 1.4153\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4157... Val Loss: 1.4087\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4214... Val Loss: 1.4085\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4382... Val Loss: 1.4079\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4424... Val Loss: 1.4061\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4211... Val Loss: 1.4060\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.3889... Val Loss: 1.4078\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4162... Val Loss: 1.4023\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3494... Val Loss: 1.4017\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3716... Val Loss: 1.4009\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3724... Val Loss: 1.4006\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.3807... Val Loss: 1.3951\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.3781... Val Loss: 1.3932\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3553... Val Loss: 1.3972\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3488... Val Loss: 1.3925\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.3810... Val Loss: 1.3858\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4299... Val Loss: 1.3896\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.3841... Val Loss: 1.3830\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.3880... Val Loss: 1.3879\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.3992... Val Loss: 1.3790\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3467... Val Loss: 1.3866\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3219... Val Loss: 1.3813\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3222... Val Loss: 1.3783\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3494... Val Loss: 1.3771\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3437... Val Loss: 1.3819\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3399... Val Loss: 1.3748\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3658... Val Loss: 1.3738\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3432... Val Loss: 1.3799\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3096... Val Loss: 1.3726\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3647... Val Loss: 1.3701\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3398... Val Loss: 1.3720\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3576... Val Loss: 1.3673\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3280... Val Loss: 1.3669\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3369... Val Loss: 1.3631\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3077... Val Loss: 1.3697\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3258... Val Loss: 1.3671\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3616... Val Loss: 1.3623\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3236... Val Loss: 1.3629\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3030... Val Loss: 1.3679\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3122... Val Loss: 1.3587\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3402... Val Loss: 1.3579\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3177... Val Loss: 1.3582\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3092... Val Loss: 1.3542\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3283... Val Loss: 1.3517\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3282... Val Loss: 1.3569\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3175... Val Loss: 1.3500\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3367... Val Loss: 1.3528\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2816... Val Loss: 1.3488\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2649... Val Loss: 1.3503\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3261... Val Loss: 1.3528\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3336... Val Loss: 1.3490\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3244... Val Loss: 1.3503\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3451... Val Loss: 1.3529\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3156... Val Loss: 1.3474\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3183... Val Loss: 1.3439\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3078... Val Loss: 1.3444\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2771... Val Loss: 1.3417\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3342... Val Loss: 1.3438\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3019... Val Loss: 1.3475\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3077... Val Loss: 1.3410\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3046... Val Loss: 1.3395\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.2818... Val Loss: 1.3416\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2879... Val Loss: 1.3419\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2731... Val Loss: 1.3386\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.2926... Val Loss: 1.3368\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3074... Val Loss: 1.3384\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2822... Val Loss: 1.3375\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3044... Val Loss: 1.3345\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2879... Val Loss: 1.3364\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.2867... Val Loss: 1.3338\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3018... Val Loss: 1.3326\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.2882... Val Loss: 1.3327\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.2975... Val Loss: 1.3327\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2740... Val Loss: 1.3301\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2763... Val Loss: 1.3298\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.2885... Val Loss: 1.3307\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2599... Val Loss: 1.3322\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2638... Val Loss: 1.3286\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.2906... Val Loss: 1.3233\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2743... Val Loss: 1.3254\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2663... Val Loss: 1.3267\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2633... Val Loss: 1.3231\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2951... Val Loss: 1.3252\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2726... Val Loss: 1.3211\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2326... Val Loss: 1.3190\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2823... Val Loss: 1.3199\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2531... Val Loss: 1.3192\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2733... Val Loss: 1.3187\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2499... Val Loss: 1.3206\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2562... Val Loss: 1.3189\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2654... Val Loss: 1.3177\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2667... Val Loss: 1.3118\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2662... Val Loss: 1.3101\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2332... Val Loss: 1.3161\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2600... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2476... Val Loss: 1.3140\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2450... Val Loss: 1.3161\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2729... Val Loss: 1.3104\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2681... Val Loss: 1.3102\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2596... Val Loss: 1.3101\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2430... Val Loss: 1.3055\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2474... Val Loss: 1.3061\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2430... Val Loss: 1.3028\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2654... Val Loss: 1.3037\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2708... Val Loss: 1.3031\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2451... Val Loss: 1.3015\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2493... Val Loss: 1.3002\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2385... Val Loss: 1.2996\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2284... Val Loss: 1.2963\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2494... Val Loss: 1.2967\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2356... Val Loss: 1.2963\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2264... Val Loss: 1.2928\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2235... Val Loss: 1.2929\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2297... Val Loss: 1.2974\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2342... Val Loss: 1.2921\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2457... Val Loss: 1.2939\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2496... Val Loss: 1.2876\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2504... Val Loss: 1.2908\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2251... Val Loss: 1.2978\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2300... Val Loss: 1.2898\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2202... Val Loss: 1.2892\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2558... Val Loss: 1.2847\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2147... Val Loss: 1.2831\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2122... Val Loss: 1.2826\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2305... Val Loss: 1.2832\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2093... Val Loss: 1.2788\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2111... Val Loss: 1.2834\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2219... Val Loss: 1.2811\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2174... Val Loss: 1.2824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.2305... Val Loss: 1.2794\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2400... Val Loss: 1.2762\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2174... Val Loss: 1.2805\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2137... Val Loss: 1.2795\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2277... Val Loss: 1.2727\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.1948... Val Loss: 1.2780\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.1991... Val Loss: 1.2806\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.1998... Val Loss: 1.2773\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.1899... Val Loss: 1.2764\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.1924... Val Loss: 1.2817\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.1872... Val Loss: 1.2781\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2212... Val Loss: 1.2736\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2564... Val Loss: 1.2736\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making  Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## priming and generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Parlinona, who went on, and said, she\n",
      "was senting her arrangements of his cletker. The strength of\n",
      "husband, had been told his beds of hating, and with the position\n",
      "there will consequent him.\n",
      "\n",
      "\"Well, that is true that you're so in a strentthing of you,\"\n",
      "he went on.\n",
      "\n",
      "\"You can't teel you, that's the much of the forest. But I wanted to\n",
      "be the same terrible times is at those passionate. The descousting of his\n",
      "fater sides to be dreamfully done that if I don't know, then an answer\n",
      "and much a chanted of all the same as a poss that they well the cheerful arranges\n",
      "who want to thought that it seems to be still so that they cannot be an\n",
      "and saw a lawy arm in the first contrary that he would carg out and\n",
      "so in society women. As it was not alone to\n",
      "himself. This is so stepping and went out of her son and the\n",
      "side and three other, and her eyes, that sideless concemiditic\n",
      "with their house shark of this sister and a personal\n",
      "paids; he was not strenct to have breathing, and that he can from his bare\n",
      "ash\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said a governor.\n",
      "\n",
      "Alexey Alexandrovitch's eyes, their straight true the prace. He\n",
      "had been talked in the same princess. He sat down again. \"I say in\n",
      "her, and I should not tell you, bringing the matter and his baby\n",
      "steps only they too was not in, and that I can'bl have been suddenly\n",
      "said at the tine of considerity and that here with her for the son\n",
      "she supposes.\n",
      "Why should you know hut and such a man who did not be and the\n",
      "candect of hastical than so much then in host for the\n",
      "same time, but, intelectual for the more and sound of the princess\n",
      "would have thought into a child of terrible of the princips. At tomatis mother is\n",
      "ill at once that were true, but I had sating to me. He doesn't be a man at once\n",
      "therefores--and how to see the problem to him?\" said Levin, smiling.\n",
      "\n",
      "After the man, world. As she wonded the chest, he houred, but he\n",
      "consequently settred him that the soft together he went out,\n",
      "and all her samisty steps saw her houre should say nothing than enecgy. The\n",
      "laster on the countess, where he seemed this and tried to discussing\n",
      "him, and she\n",
      "was strengsh in the subject that he had not been delighted of him\n",
      "at the strange and hands and shooking held of thoughts by the\n",
      "subject at the partners, and she felt this then in the sight of\n",
      "his called too, to settle the creakon transpropented and disagreest.\n",
      "\n",
      "The doctor had sent the state of the croachs with a side, the\n",
      "light of his face should be a sort of silence and shot the more of it. But that something things and\n",
      "housesell of the marshal of the morning. He could never shave him and\n",
      "taken one of the morning that the same distress as they seemed\n",
      "a good night and always driving.\n",
      "\n",
      "They were silent, took the pretty change. Though a stread had been\n",
      "before him on the contrary of the face, were this three feeling\n",
      "and softly. Then the strange sen on waiting of his face which saw the constraction.\n",
      "\n",
      "\"Well,\" all a servant was not a good news, and saying still he had not the\n",
      "servent countess. He was terrible of it.\n",
      "\n",
      "\"All will besies \n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:character-wise-rnn]",
   "language": "python",
   "name": "conda-env-character-wise-rnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
